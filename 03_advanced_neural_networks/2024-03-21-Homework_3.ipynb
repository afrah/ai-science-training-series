{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1:\n",
    "\n",
    "In this notebook, we've learned about some basic convolutional networks and trained one on CIFAR-10 images.  It did ... OK.  There is significant overfitting of this model.  There are some ways to address that, but we didn't have time to get into that in this session.\n",
    "\n",
    "Meanwhile, your homework (part 1) for this week is to try to train the model again but with a different architecture.  Change one or more of the following:\n",
    "- The number of convolutions between downsampling\n",
    "- The number of filters in each layer\n",
    "- The initial \"patchify\" layer\n",
    "- Another hyper-parameter of your choosing\n",
    "\n",
    "\n",
    "And compare your final validation accuracy to the accuracy shown here.  Can you beat the validation accuracy shown?\n",
    "\n",
    "For full credit on the homework, you need to show (via text, or make a plot) the training and validation data sets' performance (loss and accuracy) for all the epochs you train.  You also need to explain, in several sentences, what you changed in the network and why you think it makes a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afrah2/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/afrah2/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/afrah2/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import v2\n",
    "training_data = torchvision.datasets.CIFAR10(\n",
    "    root=\"/media/afrah2/MyWork/ALCF2024/ai-science-training-series/datasets/CIFAR-10/\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=v2.Compose([\n",
    "        v2.ToTensor(),\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.RandomResizedCrop(size=32, scale=[0.85,1.0], antialias=False),\n",
    "        v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "    root=\"media/afrah2/MyWork/ALCF2024/ai-science-training-series/datasets/CIFAR-10/\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "training_data, validation_data = torch.utils.data.random_split(training_data, [0.8, 0.2], generator=torch.Generator().manual_seed(55))\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# The dataloader makes our dataset iterable \n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, \n",
    "    batch_size=batch_size, \n",
    "    pin_memory=True,\n",
    "    shuffle=True, \n",
    "    num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(validation_data, \n",
    "    batch_size=batch_size, \n",
    "    pin_memory=True,\n",
    "    shuffle=False, \n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afrah2/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def preprocess(x, y):\n",
    "    # CIFAR-10 is *color* images so 3 layers!\n",
    "    return x.view(-1, 3, 32, 32).to(dev), y.to(dev)\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "\n",
    "train_dataloader = WrappedDataLoader(train_dataloader, preprocess)\n",
    "val_dataloader = WrappedDataLoader(val_dataloader, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def evaluate(dataloader, model, loss_fn, val_bar):\n",
    "    # Set the model to evaluation mode - some NN pieces behave differently during training\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader)\n",
    "    num_batches = len(dataloader)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n",
    "    with torch.no_grad():\n",
    "        # loop over all of the batches\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            # how many are correct in this batch? Tracking for accuracy \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            val_bar.update()\n",
    "            \n",
    "    loss /= num_batches\n",
    "    correct /= (size*batch_size)\n",
    "    \n",
    "    accuracy = 100*correct\n",
    "    return accuracy, loss\n",
    "\n",
    "\n",
    "def train_one_epoch(dataloader, model, loss_fn, optimizer, progress_bar):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # backward pass calculates gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # take one step with these gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # resets the gradients \n",
    "        optimizer.zero_grad()      \n",
    "\n",
    "        progress_bar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Downsampler(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, shape, stride=2 ,kernel_size = 3 , padding = 1):\n",
    "        super(Downsampler, self).__init__()\n",
    "\n",
    "        self.norm = nn.LayerNorm([in_channels, *shape])\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        layers=[\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_channels,out_channels,kernel_size,stride,padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(2)]\n",
    "        \n",
    "        self.downsample=nn.Sequential(*layers)\n",
    "\n",
    "        # self.downsample = nn.Conv2d(\n",
    "        #     in_channels=in_channels, \n",
    "        #     out_channels=out_channels,\n",
    "        #     kernel_size = stride,\n",
    "        #     stride = stride,\n",
    "        # )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "\n",
    "\n",
    "        return self.downsample(self.norm(inputs))\n",
    "        \n",
    "        \n",
    "\n",
    "class ConvNextBlock(nn.Module):\n",
    "    \"\"\"This block of operations is loosely based on this paper:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels, shape):\n",
    "        super(ConvNextBlock, self).__init__()\n",
    "\n",
    "        # Depthwise, seperable convolution with a large number of output filters:\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                                     out_channels=in_channels, \n",
    "                                     groups=in_channels,\n",
    "                                     kernel_size=[4,4],\n",
    "                                     padding='same' )\n",
    "\n",
    "        self.norm = nn.LayerNorm([in_channels, *shape])\n",
    "\n",
    "        # Two more convolutions:\n",
    "        self.conv2 = nn.Conv2d(in_channels=in_channels, \n",
    "                                     out_channels=4*in_channels,\n",
    "                                     kernel_size=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=4*in_channels, \n",
    "                                     out_channels=in_channels,\n",
    "                                     kernel_size=1\n",
    "                                     )\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "\n",
    "        # The normalization layer:\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # The non-linear activation layer:\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # This makes it a residual network:\n",
    "        return x + inputs\n",
    "    \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, n_initial_filters, n_stages, blocks_per_stage):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        # This is a downsampling convolution that will produce batches of output.\n",
    "\n",
    "        # This is similar to what vision transformers do to tokenize the images.\n",
    "        self.stem = nn.Conv2d(in_channels=3,\n",
    "                                    out_channels=n_initial_filters,\n",
    "                                    kernel_size=1,\n",
    "                                    stride=1)\n",
    "        \n",
    "        current_shape = [32, 32]\n",
    "\n",
    "        self.norm1 = nn.LayerNorm([n_initial_filters,*current_shape])\n",
    "        # self.norm1 = WrappedLayerNorm()\n",
    "\n",
    "        current_n_filters = n_initial_filters\n",
    "        \n",
    "        self.layers = nn.Sequential()\n",
    "        for i, n_blocks in enumerate(range(n_stages)):\n",
    "            # Add a convnext block series:\n",
    "            for _ in range(blocks_per_stage):\n",
    "                self.layers.append(ConvNextBlock(in_channels=current_n_filters, shape=current_shape))\n",
    "            # Add a downsampling layer:\n",
    "            if i != n_stages - 1:\n",
    "                # Skip downsampling if it's the last layer!\n",
    "                self.layers.append(Downsampler(\n",
    "                    in_channels=current_n_filters, \n",
    "                    out_channels=2*current_n_filters,\n",
    "                    shape = current_shape,\n",
    "                    )\n",
    "                )\n",
    "                # Double the number of filters:\n",
    "                current_n_filters = 2*current_n_filters\n",
    "                # Cut the shape in half:\n",
    "                current_shape = [ cs // 2 for cs in current_shape]\n",
    "            \n",
    "\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LayerNorm(current_n_filters),\n",
    "            nn.Linear(current_n_filters, 10)\n",
    "        )\n",
    "        # self.norm2 = nn.InstanceNorm2d(current_n_filters)\n",
    "        # # This brings it down to one channel / class\n",
    "        # self.bottleneck = nn.Conv2d(in_channels=current_n_filters, out_channels=10, \n",
    "        #                                   kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x = self.stem(inputs)\n",
    "        # Apply a normalization after the initial patching:\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Apply the main chunk of the network:\n",
    "        x = self.layers(x)\n",
    "\n",
    "        # Normalize and readout:\n",
    "        x = nn.functional.avg_pool2d(x, x.shape[2:])\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "        # x = self.norm2(x)\n",
    "        # x = self.bottleneck(x)\n",
    "\n",
    "        # # Average pooling of the remaining spatial dimensions (and reshape) makes this label-like:\n",
    "        # return nn.functional.avg_pool2d(x, kernel_size=x.shape[-2:]).reshape((-1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Classifier(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchinfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary(model, input_size\u001b[38;5;241m=\u001b[39m(batch_size, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mcuda(device))\n",
      "File \u001b[0;32m~/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mcuda(device))\n",
      "File \u001b[0;32m~/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/cuda/__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    246\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_init()\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "\n",
    "model = Classifier(64, 4, 2)\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "print(summary(model, input_size=(batch_size, 3, 32, 32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03d970002244d9f9104afd3a545fe6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 0:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in pin memory thread for device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/afrah2/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 34, in do_one_step\n    data = pin_memory(data, device)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/afrah2/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 70, in pin_memory\n    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/afrah2/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 70, in <listcomp>\n    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/afrah2/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 55, in pin_memory\n    return data.pin_memory(device)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader), position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m train_bar:\n\u001b[0;32m---> 11\u001b[0m         train_one_epoch(train_dataloader, model, loss_fn, optimizer, train_bar)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# checking on the training loss and accuracy once per epoch\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader), position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidate (train) Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m train_eval:\n",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(dataloader, model, loss_fn, optimizer, progress_bar)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_one_epoch\u001b[39m(dataloader, model, loss_fn, optimizer, progress_bar):\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     34\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mWrappedDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl:\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39mb))\n",
      "File \u001b[0;32m~/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in pin memory thread for device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/afrah2/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 34, in do_one_step\n    data = pin_memory(data, device)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/afrah2/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 70, in pin_memory\n    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/afrah2/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 70, in <listcomp>\n    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/afrah2/anaconda3/envs/torchdiffeq/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py\", line 55, in pin_memory\n    return data.pin_memory(device)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "epochs = 30\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "validation_loss_history = []\n",
    "validation_accuracy_hisstory = []\n",
    "for j in range(epochs):\n",
    "    with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f\"Train Epoch {j}\") as train_bar:\n",
    "        train_one_epoch(train_dataloader, model, loss_fn, optimizer, train_bar)\n",
    "    \n",
    "    # checking on the training loss and accuracy once per epoch\n",
    "        \n",
    "    with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f\"Validate (train) Epoch {j}\") as train_eval:\n",
    "        acc, loss = evaluate(train_dataloader, model, loss_fn, train_eval)\n",
    "        train_loss_history.append(loss)\n",
    "        train_accuracy_history.append(acc)\n",
    "\n",
    "        print(f\"Epoch {j}: training loss: {loss:.3f}, accuracy: {acc:.3f}\")\n",
    "    with tqdm(total=len(val_dataloader), position=0, leave=True, desc=f\"Validate Epoch {j}\") as val_bar:\n",
    "    \n",
    "        acc_val, loss_val = evaluate(val_dataloader, model, loss_fn, val_bar)\n",
    "        validation_loss_history.append(loss)\n",
    "        validation_accuracy_hisstory.append(loss)\n",
    "\n",
    "        print(f\"Epoch {j}: validation loss: {loss_val:.3f}, accuracy: {acc_val:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_accuracy_history, label='training accuracy')\n",
    "plt.plot(validation_accuracy_hisstory, label= 'validation accuracy')\n",
    "plt.title('Accuracy per epoch')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.plot(validation_loss_history, label= 'Validation Loss')\n",
    "plt.title('Loss per epoch')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchdiffeq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
